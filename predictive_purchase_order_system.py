# -*- coding: utf-8 -*-
"""Predictive Purchase Order System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18_yjmR1CxgjOUPsua4CcE1fBB2vHk4PM

**Dominos - Predictive Purchase Order System**

# Data cleaning and preprocessing
"""

# !pip uninstall -y numpy pmdarima
# !pip install numpy==1.24.4
# !pip install pmdarima --no-cache-dir

# Commented out IPython magic to ensure Python compatibility.
#Required library
import numpy as np
import pandas as pd
import os
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
from sklearn.metrics import mean_absolute_percentage_error as mape
from sklearn import datasets
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from datetime import datetime
pd.options.display.float_format = '{:.2f}'.format
from pmdarima import auto_arima

df=pd.read_csv('/content/Pizza_Sale - pizza_sales.csv')
df.head()

df.info()

df.shape

df.describe()

#null values count
df.isnull().sum()

#last five rows in dataframe
df.tail()

#show the null values in pizza_name_id
df[df['pizza_name_id'].isnull()]

"""**Handling Missing Values Pizza Sales Data**"""

#pizza name_id_mapping
name_id_mapping = df[['pizza_name', 'pizza_name_id']].dropna().drop_duplicates()
name_id_mapping = name_id_mapping.set_index('pizza_name')['pizza_name_id'].to_dict()

df['pizza_name_id'] = df['pizza_name_id'].fillna(df['pizza_name'].map(name_id_mapping))

df.info()

#show the null values in pizza_name

df[df['pizza_name'].isnull()]

#pizza_name_mapping
pizza_name_mapping = df[['pizza_name_id', 'pizza_name']].dropna().drop_duplicates()
pizza_name_mapping = pizza_name_mapping.set_index('pizza_name_id')['pizza_name'].to_dict()

df['pizza_name'] = df['pizza_name'].fillna(df['pizza_name_id'].map(pizza_name_mapping))

#show the null values in total_price

df[df['total_price'].isnull()]

df['total_price'].fillna(df['quantity']*df['unit_price'],inplace=True)

#show the null values in pizza_category
df[df['pizza_category'].isnull()]

#pizza_category_mapping
pizza_category_mapping = df[['pizza_name', 'pizza_category']].dropna().drop_duplicates()
pizza_category_mapping = pizza_category_mapping.set_index('pizza_name')['pizza_category'].to_dict()

df['pizza_category'] = df['pizza_category'].fillna(df['pizza_name'].map(pizza_category_mapping))

df.info()

#show the null values in pizza_ingredients
df[df['pizza_ingredients'].isnull()]

#pizza_ingredients_mapping
pizza_ingredients_mapping = df[['pizza_name', 'pizza_ingredients']].dropna().drop_duplicates()
pizza_ingredients_mapping = pizza_ingredients_mapping.set_index('pizza_name')['pizza_ingredients'].to_dict()

df['pizza_ingredients'] = df['pizza_ingredients'].fillna(df['pizza_name'].map(pizza_ingredients_mapping))

df.info()

df.isnull().sum()

#check the duplicate values
df[df.duplicated(keep=False)]

"""**Handling Missing Values in Pizza Ingredients data**"""

ingradients_df=pd.read_csv('/content/Pizza_ingredients - Pizza_ingredients.csv')
ingradients_df.head()

ingradients_df.shape

ingradients_df.describe()

ingradients_df.info()

ingradients_df.isnull().sum()

#show the null values in Items_Qty_In_Grams
ingradients_df[ingradients_df['Items_Qty_In_Grams'].isnull()]

filtered_data= ingradients_df[ingradients_df['pizza_name_id']=='brie_carre_s']
filtered_data

filtered_data= ingradients_df[ingradients_df['pizza_name_id']=='hawaiian_l']
filtered_data

filtered_data= ingradients_df[ingradients_df['pizza_name_id']=='hawaiian_m']
filtered_data

filtered_data= ingradients_df[ingradients_df['pizza_name_id']=='hawaiian_s']
filtered_data

mean_qty=ingradients_df.groupby('pizza_name_id')['Items_Qty_In_Grams'].mean()

print(mean_qty)

ingradients_df['Items_Qty_In_Grams'] = ingradients_df['Items_Qty_In_Grams'].fillna(ingradients_df['pizza_name_id'].map(mean_qty))

ingradients_df.info()

#total ingredients
count_ingredients=len(ingradients_df['pizza_ingredients'].unique())
print(count_ingredients)

#finding duplicate values
ingradients_df[ingradients_df.duplicated(keep=False)]

"""**Merge two tables**"""

merge_data= pd.merge(df,ingradients_df, how="left", on="pizza_name_id")

merge_data

merge_data.info()

merge_data[['pizza_name_x','pizza_name_y']]

merge_data[['pizza_ingredients_x','pizza_ingredients_y']]

#drop the columns
merge_data.drop(columns=['pizza_name_y','pizza_ingredients_y'],inplace=True)
#rename the columns
merge_data.rename(columns={'pizza_name_x':'pizza_name','pizza_ingredients_x':'pizza_ingredients'},inplace=True)

#total columns
merge_data.columns

merge_data.info()

#duplicate rows in merge data
merge_data[merge_data.duplicated(keep=False)]

#drop duplicate rows in merge data

merge_data.drop_duplicates(inplace=True)

#converting dtype
def parse_dates(date):
    for fmt in ("%d-%m-%Y", "%d/%m/%Y", "%Y-%m-%d"):  # Add more if needed
        try:
            return pd.to_datetime(date, format=fmt)
        except ValueError:
            continue
    return pd.NaT  # If all fail
merge_data['order_date'] = merge_data['order_date'].apply(parse_dates)

merge_data[merge_data['order_date'].isnull()]

#today date
import datetime as dt
today=dt.date.today()
today

#check the future dates
merge_data[merge_data['order_date'].dt.date > today]

"""# **Feature Engineering**








"""

merge_data['year']=merge_data['order_date'].dt.year
merge_data['Month']=merge_data['order_date'].dt.month
merge_data['Day']=merge_data['order_date'].dt.day

merge_data['day_of_week']=merge_data['order_date'].dt.isocalendar().week

merge_data

#reset the index
merge_data.reset_index(inplace = True, drop = True)

merge_data

!pip install holidays
import holidays

us_holidays=holidays.US()
# Creating a 'holiday' column
merge_data['holiday'] = merge_data['order_date'].apply(lambda x: 1 if x in us_holidays else 0)

merge_data[['order_date','holiday']].head()

merge_data[['order_date','holiday']].tail()

merge_data['Promotion'] = merge_data['order_date'].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)

merge_data[['order_date','Promotion']].head()

#pizza count of daily sales
daily_sales = merge_data.groupby('order_date')['quantity'].sum()
daily_sales



# Filter data for week 7
week_7_sales = merge_data[merge_data['day_of_week']==7]  # Days 0 to 6 (Monday to Sunday)

# Group by pizza name and sum the quantity sold
week_7_pizza_sales = week_7_sales.groupby('pizza_name')['quantity'].sum().reset_index()

# Sort by highest sales
week_7_pizza_sales = week_7_pizza_sales.sort_values('quantity', ascending=False)

# Display week 7 pizza sales
print("\nüçï Pizza Sales Count for Week 7:")
print(week_7_pizza_sales)

"""# **Exploratory Data Analysis (EDA):**"""

ax=merge_data['pizza_name'].value_counts().plot.bar(rot=0)
plt.ylabel('Count')
plt.xlabel('Pizza¬†Category')
plt.xticks(rotation=90, ha='right')
plt.title('Distribution¬†of¬†Pizza¬†Categories')
plt.show()

# Sort data to get top 10 pizzas by total_price
top_10_pizzas = merge_data.groupby('pizza_name')['total_price'].sum().reset_index()
top_10_pizzas = top_10_pizzas.sort_values(by='total_price', ascending=False).head(10)

# Plot
plt.figure(figsize=(12, 8))
sns.barplot(x='total_price', y='pizza_name', data=top_10_pizzas, palette='Set2')
plt.title('Top 10 Pizzas by Total Price')
plt.xlabel('Total Price')
plt.ylabel('Pizza Name')
plt.show()

sns.set(style="whitegrid")  # Setting the style to whitegrid for a clean background

plt.figure(figsize=(12, 6))  # Setting the figure size
# To plot the total quantity of pizzas sold each day, you need to aggregate the data first.
daily_sales = merge_data.groupby('order_date')['total_price'].sum().reset_index()
# Now you can plot 'daily_sales' against 'order_date'
sns.lineplot(data=daily_sales, x='order_date', y='total_price', label='Total Price', color='blue')

# Adding labels and title
plt.xlabel('Order Date')
plt.ylabel('Quantity Sold') # Changed y-axis label to reflect the data being plotted
plt.title('Sales Trend Over the Period of  Time') # Changed title to be more descriptive

plt.show()

# data used in this section
temp_data = merge_data.groupby(['Month'])[['total_price','quantity']].sum().reset_index()

#
fig, axes = plt.subplots(2, 1, figsize = (40,25))
fig.subplots_adjust(hspace=.3)

sns.barplot(x='Month', y='total_price', data=temp_data, ax=axes[0])
axes[0].set_xlabel(axes[0].get_xlabel(), size=30)
axes[0].set_ylabel(axes[0].get_ylabel(), size=30)
axes[0].set_xticklabels(axes[0].get_xticklabels(), size=30)
axes[0].bar_label(axes[0].containers[0], fmt='%.2f', size=25)
axes[0].set_title('Total Sales per Month', size= 40)

# -------

#
sns.barplot(x='Month', y='quantity', data=temp_data, ax=axes[1])
axes[1].set_xlabel(axes[1].get_xlabel(), size=30)
axes[1].set_ylabel(axes[1].get_ylabel(), size=30)
axes[1].set_xticklabels(axes[1].get_xticklabels(), size=30)
axes[1].bar_label(axes[1].containers[0], fmt='%.2f', size=25)
axes[1].set_title('Total Quantity Ordered per Month', size= 40)



!pip install ydata-profiling

# For ydata_profiling (latest)
from ydata_profiling import ProfileReport

# If you're using older pandas-profiling (pre-2023)
# from pandas_profiling import ProfileReport

profile = ProfileReport(merge_data)
profile

sns.countplot(merge_data, x="pizza_category",hue="Promotion")

sns.lineplot(data=merge_data, x="Day", y="quantity")

sns.barplot(x='holiday', y='total_price', data=merge_data, palette='viridis')
plt.title('Sales on Holidays vs Non-Holidays')
plt.show()

sns.barplot(x='Promotion', y='total_price', data=merge_data, palette='magma')
plt.title('Sales during Promotional Periods vs Non-Promotional Periods')
plt.show()

matrix = merge_data[['quantity', 'unit_price', 'total_price']].corr()

# plotting correlation matrix
sns.heatmap(matrix, cmap="Greens", annot=True)

--

"""# **Model Selection**

**ARIMA model**

AR (p) : Auto Regressive
I (d) : Integrated
MA (q) : Moving Average
(p,d,q) is known as the order of the ARIMA model. Values of these parameters are based on the above mentioned models.

p : Number of auto regressive terms.

d : Number of differencing orders required to make the time series stationary.

q : Number of lagged forecast errors in the prediction equation.

ARIMA MODEL
"""

# Assuming 'merge_data' has 'order_date' and 'quantity'
merge_data['order_date'] = pd.to_datetime(merge_data['order_date'])

# Group by week
weekly_sales = merge_data.groupby(merge_data['order_date'].dt.to_period('W'))['quantity'].sum()
weekly_sales.index = weekly_sales.index.to_timestamp()  # convert PeriodIndex to Timestamp
weekly_sales = weekly_sales.sort_index()
weekly_sales = weekly_sales.astype(float)

#train test split
train_size = int(len(weekly_sales) * 0.8)
train, test = weekly_sales[0:train_size], weekly_sales[train_size:]

def Mape(actual, predicted):
    return np.mean(np.abs((actual - predicted) / actual)) * 100

# evaluate an ARIMA model for a given order (p,d,q)
def evaluate_arima_model(X,arima_order):
    train_size = int(len(X) * 0.8)
    train, test = X[0:train_size],X[train_size:]
    history = [x for x in train]

    model = ARIMA(history, order=arima_order)
    model_fit = model.fit()
    forecast = model_fit.forecast(steps=len(test))
    forecast_series = pd.Series(forecast, index=test.index)
    mape = Mape(test, forecast)
    return mape

# grid search
def evaluate_models(dataset, p_values, d_values, q_values):
    dataset = dataset.astype('float32')
    best_score, best_cfg = float("inf"), None
    for p in p_values:
        for d in d_values:
            for q in q_values:
                order = (p,d,q)
                try:
                    mape = evaluate_arima_model(dataset, order)
                    if mape < best_score:
                        best_score, best_cfg = mape, order
                    # print('ARIMA%s MAPE=%.3f' % (order, mape))
                except:
                    continue
    print('Best ARIMA%s MAPE=%.3f' % (best_cfg, best_score))


p_values = [0, 1, 2, 3, 4, 5,6]
d_values = range(0, 2)
q_values = range(0, 6)

warnings.filterwarnings("ignore")
evaluate_models(weekly_sales, p_values, d_values, q_values)

model = ARIMA(train ,order=(3,0,3))
model_fit = model.fit()
forecast = model_fit.forecast(steps=len(test))

# Print forecast series
forecast_series = pd.Series(forecast, index=test.index)
print(forecast_series)

#plot the data
plt.figure(figsize=(12, 6))
plt.plot(test.index, test.values, label='Actual Sales', color='blue', marker='o')
plt.plot(forecast_series.index, forecast_series, label='Predicted Sales', color='orange', linestyle='--', marker='x')
plt.title('ARIMA Predictions vs Actual Sales')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""**SARIMAX MODEL**

SARIMA (Seasonal AutoRegressive Integrated Moving Average) is a statistical model used for forecasting time series data that exhibits both trends and seasonal patterns.

SARIMA(p, d, q)(P, D, Q, s)

p, d, q: Non-seasonal autoregressive, differencing, and moving average terms

P, D, Q: Seasonal autoregressive, differencing, and moving average terms

s: The number of time steps in a season (e.g., 7 for weekly seasonality)
"""

#train test split
train_size = int(len(weekly_sales) * 0.8)
train, test = weekly_sales[0:train_size], weekly_sales[train_size:]

def mape(actual, predicted):
    return np.mean(np.abs((actual - predicted) / actual)) * 100


# Best SARIMA Model Training and Output
def best_sarima_model(train, test):
    model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)).fit(disp=False)
    predictions = model.forecast(steps=len(test))
    sarima_mape = mape(test, predictions)

    print(f"Best SARIMA Model MAPE: {sarima_mape:.4f}")
    return predictions, sarima_mape

# Train and evaluate the SARIMA model
sarima_predictions, sarima_mape_score = best_sarima_model(train, test)

# Formating the predictions for display
sarima_predictions = pd.Series(sarima_predictions, index=test.index)

print("Predictions:")
print(sarima_predictions)

#plot the data
plt.figure(figsize=(12, 6))
plt.plot(test.index, test.values, label='Actual Sales', color='blue', marker='o')
plt.plot(sarima_predictions.index, sarima_predictions, label='Predicted Sales', color='orange', linestyle='--', marker='x')
plt.title('SARIMA Predictions vs Actual Sales')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

"""LSTM

LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks.
"""

#train test split
train_size = int(len(weekly_sales) * 0.8)
train, test = weekly_sales[0:train_size], weekly_sales[train_size:]

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler

!pip install pandas numpy matplotlib scikit-learn tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_percentage_error as mape
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# -----------------------------
# Import Libraries
# -----------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_absolute_percentage_error as mape

# -----------------------------
# Weekly Sales Preparation
# -----------------------------
def prepare_weekly_sales(df):
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
    df = df.dropna(subset=['order_date'])
    weekly = df.groupby(df['order_date'].dt.to_period('W').apply(lambda r: r.start_time))['quantity'].sum().reset_index()
    weekly.columns = ['week_start', 'quantity']
    return weekly.sort_values('week_start')

# -----------------------------
# Create Sequences for LSTM
# -----------------------------
def create_sequences(data, time_steps):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(data[['quantity']])

    X, y = [], []
    for i in range(time_steps, len(scaled)):
        X.append(scaled[i-time_steps:i])
        y.append(scaled[i])

    # Convert to float32 numpy arrays
    X = np.array(X).astype(np.float32)
    y = np.array(y).astype(np.float32)

    split = int(len(X) * 0.8)
    return X[:split], y[:split], X[split:], y[split:], scaler, data['week_start'][time_steps+split:]

# -----------------------------
# LSTM Forecast Function
# -----------------------------
def forecast_lstm(X_train, y_train, X_test, y_test, scaler, dates):
    model = Sequential([
        LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])), # The input shape should be (timesteps, features)
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

    preds = model.predict(X_test)
    preds_inv = scaler.inverse_transform(preds)  # Inverse transform to get actual sales values
    actuals_inv = scaler.inverse_transform(y_test.reshape(-1, 1)) # Reshape y_test for inverse transform

    print(f"MAPE: {mape(actuals_inv, preds_inv):.4f}")
    return pd.Series(preds_inv.flatten(), index=dates[:len(preds)]), actuals_inv # Flatten preds_inv for Series

# -----------------------------
# Run the Pipeline
# -----------------------------
# Replace `merge_data` with your actual DataFrame
weekly_sales = prepare_weekly_sales(merge_data)
X_train, y_train, X_test, y_test, scaler, test_dates = create_sequences(weekly_sales, time_steps=3)
forecast, actuals = forecast_lstm(X_train, y_train, X_test, y_test, scaler, test_dates)

# -----------------------------
# Plot Forecast vs Actual
# -----------------------------
plt.figure(figsize=(10, 5))
plt.plot(test_dates[:len(actuals)], actuals, label='Actual', marker='o')
plt.plot(forecast.index, forecast.values, label='Forecast', marker='x', linestyle='--')
plt.xlabel('Week')
plt.ylabel('Sales Quantity')
plt.title('Weekly Sales Forecast vs Actual')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

!pip install --upgrade jax jaxlib

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.preprocessing import MinMaxScaler
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense
# from sklearn.metrics import mean_absolute_percentage_error as mape

# Prepare weekly sales
def prepare_weekly_sales(df):
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
    df = df.dropna(subset=['order_date'])
    weekly = df.groupby(df['order_date'].dt.to_period('W').apply(lambda r: r.start_time))['quantity'].sum().reset_index()
    weekly.columns = ['week_start', 'quantity']
    return weekly.sort_values('week_start')

# Prepare data for LSTM
def create_sequences(data, time_steps):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(data[['quantity']])
    X, y = [], []
    for i in range(time_steps, len(scaled)):
        X.append(scaled[i-time_steps:i])
        y.append(scaled[i])
    X, y = np.array(X), np.array(y)
    split = int(len(X) * 0.8)
    return X[:split], y[:split], X[split:], y[split:], scaler, data['week_start'][time_steps+split:]

# Train model and forecast
def forecast_lstm(X_train, y_train, X_test, y_test, scaler, dates):
    model = Sequential([LSTM(50, input_shape=(X_train.shape[1], 1)), Dense(1)])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    preds = scaler.inverse_transform(model.predict(X_test))
    actuals = scaler.inverse_transform(y_test)
    print(f"MAPE: {mape(actuals, preds):.4f}")
    return pd.Series(preds.flatten(), index=dates[:len(preds)]), actuals

# Run everything
weekly_sales = prepare_weekly_sales(merge_data)
X_train, y_train, X_test, y_test, scaler, test_dates = create_sequences(weekly_sales, time_steps=3)
forecast, actuals = forecast_lstm(X_train, y_train, X_test, y_test, scaler, test_dates)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(test_dates[:len(actuals)], actuals, label='Actual', marker='o')
plt.plot(forecast.index, forecast.values, label='Forecast', marker='x', linestyle='--')
plt.xlabel('Week')
plt.ylabel('Sales Quantity')
plt.title('Weekly Sales Forecast vs Actual')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

!pip install tensorflow --upgrade --quiet

# Prepare weekly sales
def prepare_weekly_sales(df):
    df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')
    df = df.dropna(subset=['order_date'])
    weekly = df.groupby(df['order_date'].dt.to_period('W').apply(lambda r: r.start_time))['quantity'].sum().reset_index()
    weekly.columns = ['week_start', 'quantity']
    return weekly.sort_values('week_start')

# Prepare data for LSTM
def create_sequences(data, time_steps):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(data[['quantity']])
    X, y = [], []
    for i in range(time_steps, len(scaled)):
        X.append(scaled[i-time_steps:i])
        y.append(scaled[i])
    X, y = np.array(X), np.array(y)
    split = int(len(X) * 0.8)
    return X[:split], y[:split], X[split:], y[split:], scaler, data['week_start'][time_steps+split:]

# Train model and forecast
def forecast_lstm(X_train, y_train, X_test, y_test, scaler, dates):
    model = Sequential([LSTM(50, input_shape=(X_train.shape[1], 1)), Dense(1)])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    preds = scaler.inverse_transform(model.predict(X_test))
    actuals = scaler.inverse_transform(y_test)
    print(f"MAPE: {mape(actuals, preds):.4f}")
    return pd.Series(preds.flatten(), index=dates[:len(preds)]), actuals

# Run everything
weekly_sales = prepare_weekly_sales(merge_data)
X_train, y_train, X_test, y_test, scaler, test_dates = create_sequences(weekly_sales, time_steps=3)
forecast, actuals = forecast_lstm(X_train, y_train, X_test, y_test, scaler, test_dates)

# Plot
plt.figure(figsize=(12, 6))
plt.plot(test_dates[:len(actuals)], actuals, label='Actual',color='blue', marker='o')
plt.plot(forecast.index, forecast.values, label='Forecast', marker='x',color='orange', linestyle='--')
plt.xlabel('Week')
plt.ylabel('Sales Quantity')
plt.title('Weekly Sales Forecast vs Actual')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

forecast_series

"""**Prophet**

Prophet is an open-source time series forecasting library developed by Facebook (Meta).It is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data.

**Model:** Additive model ‚Äì y(t) = trend + seasonality + holidays + error
"""

# === STEP 1: Clean and Prepare Weekly Sales Data ===
def prepare_weekly_sales(merge_data):
    merge_data['order_date'] = pd.to_datetime(merge_data['order_date'], errors='coerce')
    merge_data = merge_data.dropna(subset=['order_date'])
    weekly_sales = merge_data.groupby(merge_data['order_date'].dt.to_period('W').apply(lambda r: r.start_time))['quantity'].sum().reset_index()
    weekly_sales = weekly_sales.rename(columns={'order_date': 'week_start'})
    weekly_sales = weekly_sales.sort_values('week_start').reset_index(drop=True)
    return weekly_sales

# Convert from merge_data
weekly_sales= prepare_weekly_sales(merge_data)

# === Prepare data for Prophet ===
prophet_df = weekly_sales.rename(columns={'week_start': 'ds', 'quantity': 'y'})

# Train-test split (e.g., 80-20)
split = int(len(prophet_df) * 0.8)
train_df = prophet_df[:split]
test_df = prophet_df[split:].reset_index(drop=True)

# Train Prophet model
model = Prophet()
model.fit(train_df)

# Predict on test period (not future)
future = test_df[['ds']]  # Only use the dates from test set
forecast = model.predict(future)

# Extract predicted values
test_df['Prophet_Predictions'] = forecast['yhat']

# === Evaluate with MAPE ===
prophet_mape = mape(test_df['y'], test_df['Prophet_Predictions'])
print(f"Prophet MAPE on test data: {prophet_mape:.4f}")

# === Optional: Plot Actual vs Predicted ===
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')
plt.plot(test_df['ds'], test_df['Prophet_Predictions'], label='Predicted', marker='x', linestyle='--')
plt.title('Prophet: Actual vs Predicted Weekly Sales')
plt.xlabel('Week')
plt.ylabel('Sales Quantity')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

test_df['Prophet_Predictions']

""" **Regression model**"""

# === Convert week_start to datetime ===
weekly_sales['week_start'] = pd.to_datetime(weekly_sales['week_start'])

# === Feature Engineering ===
weekly_sales['weekofyear'] = weekly_sales['week_start'].dt.isocalendar().week
weekly_sales['month'] = weekly_sales['week_start'].dt.month
weekly_sales['quarter'] = weekly_sales['week_start'].dt.quarter
weekly_sales['year'] = weekly_sales['week_start'].dt.year
weekly_sales['week_num'] = range(len(weekly_sales))  # Sequential index as feature

# === Train-Test Split (80-20) ===
train_size = int(len(weekly_sales) * 0.8)
train, test = weekly_sales[0:train_size], weekly_sales[train_size:]

# === Define Features (X) and Target (y) ===
X_train = train[['weekofyear', 'month', 'quarter', 'year', 'week_num']]
y_train = train['quantity']
X_test = test[['weekofyear', 'month', 'quarter', 'year', 'week_num']]
y_test = test['quantity']

# === Train Linear Regression Model ===
model = LinearRegression()
model.fit(X_train, y_train)

# === Predict ===
y_pred = model.predict(X_test)
print(y_pred)
# === Evaluate Model ===
mape_score = mape(y_test, y_pred)
print(f"Linear Regression MAPE: {mape_score * 100:.2f}%")

# === Plot Actual vs Predicted ===
plt.figure(figsize=(12, 6))
plt.plot(test['week_start'], y_test, label='Actual Sales', color='blue', marker='o')
plt.plot(test['week_start'], y_pred, label='Predicted Sales', color='green', linestyle='--', marker='x')
plt.title('Linear Regression - Weekly Pizza Sales Forecast')
plt.xlabel('Week Start Date')
plt.ylabel('Sales Quantity')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.xticks(rotation=45)
plt.show()

"""**Predict pizza sales for the next one week**

ARIMA
"""

# Step 1: Prepare weekly sales data by pizza type
def prepare_weekly_sales_by_pizza(df):
    df['order_date'] = pd.to_datetime(df['order_date'])
    df['week'] = df['order_date'].dt.to_period('W').apply(lambda r: r.start_time)
    weekly_sales_by_pizza = df.groupby(['week', 'pizza_name_id'])['quantity'].sum().reset_index()
    weekly_sales_by_pizza.set_index('week', inplace=True)
    return weekly_sales_by_pizza

# Step 2: ARIMA forecast function for each pizza type
def forecast_sales_arima(pizza_type, train_series, periods=1):
    model = ARIMA(train_series, order=(1, 1, 1))
    model_fit = model.fit()
    forecast = model_fit.forecast(steps=periods)
    return pizza_type, forecast.tolist()

# Step 3: Loop over all pizza types and forecast using ARIMA
def forecast_next_week_arima(weekly_df, periods=1):
    forecasts = {}
    last_week = weekly_df.index.max()
    next_week_start = last_week + pd.Timedelta(weeks=1)
    next_week_end = next_week_start + pd.Timedelta(days=6)

    for pizza_type in weekly_df['pizza_name_id'].unique():
        df = weekly_df[weekly_df['pizza_name_id'] == pizza_type]
        train = df['quantity']
        if len(train) >= 5:
            _, forecast = forecast_sales_arima(pizza_type, train, periods)
            forecasts[pizza_type] = int(forecast[0])
    return forecasts, next_week_start, next_week_end

# Run this after loading your `pizza_sales` dataset
pizza_sales_weekly_by_pizza = prepare_weekly_sales_by_pizza(merge_data)

# ARIMA Forecast
arima_forecasts, arima_start, arima_end = forecast_next_week_arima(pizza_sales_weekly_by_pizza)
print(f"\nüìà ARIMA Forecast for {arima_start.date()} to {arima_end.date()}")
for pizza, val in arima_forecasts.items():
    print(f"{pizza}: {val} pizzas")
total_arima = sum(arima_forecasts.values())
print(f"\nüßæ Total forecasted pizzas (ARIMA): {total_arima} pizzas")

"""SARIMA"""

# Step 4: SARIMA forecast function for each pizza type
def forecast_sales_sarima(pizza_type, train_series, periods=1):
    model = SARIMAX(train_series, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))
    model_fit = model.fit(disp=False)
    forecast = model_fit.forecast(steps=periods)
    return pizza_type, forecast.tolist()

# Step 5: Loop over all pizza types and forecast using SARIMA
def forecast_next_week_sarima(weekly_df, periods=1):
    forecasts = {}
    last_week = weekly_df.index.max()
    next_week_start = last_week + pd.Timedelta(weeks=1)
    next_week_end = next_week_start + pd.Timedelta(days=6)

    for pizza_type in weekly_df['pizza_name_id'].unique():
        df = weekly_df[weekly_df['pizza_name_id'] == pizza_type]
        train = df['quantity']
        if len(train) >= 52:  # SARIMA needs at least one full season
            _, forecast = forecast_sales_sarima(pizza_type, train, periods)
            forecasts[pizza_type] = int(forecast[0])
    return forecasts, next_week_start, next_week_end

# SARIMA Forecast
sarima_forecasts, sarima_start, sarima_end = forecast_next_week_sarima(pizza_sales_weekly_by_pizza)
print(f"\nüìâ SARIMA Forecast for {sarima_start.date()} to {sarima_end.date()}")
for pizza, val in sarima_forecasts.items():
    print(f"{pizza}: {val} pizzas")
# Total pizzas forecasted by SARIMA
total_sarima = sum(sarima_forecasts.values())
print(f"üßæ Total forecasted pizzas (SARIMA): {total_sarima} pizzas")



"""**Model Comparision**"""

# Data
MODEL = ['SARIMA', 'ARIMA', 'REGRESSION_MODEL', 'PROPHET', 'LSTM']
VALUE = [18.48, 18.94, 18.99, 19.62, 21.19]

# Set Seaborn theme
sns.set_theme(style="whitegrid")

# Create figure
plt.figure(figsize=(10,6))
barplot = sns.barplot(x=MODEL, y=VALUE, palette="mako", edgecolor='black')

# Annotate bars neatly above
for index, value in enumerate(VALUE):
    plt.text(index, value + 0.3, f'{value:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=12)

# Titles and labels
plt.title('Model Error Comparison ', fontsize=16, fontweight='bold', color='navy')
plt.xlabel('Model', fontsize=14)
plt.ylabel('Error Value', fontsize=14)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()



"""**Model Evaluation**

"""

import statsmodels.api as sm # Importing the statsmodels library with the alias 'sm'

model = sm.tsa.statespace.SARIMAX(train,order = (1,1,1),seasonal_order = (1,1,1,7))
model_fit = model.fit()
print(model_fit.summary())

# Useing auto_arima to find the best parameters
auto_model = auto_arima(train['quantity'], seasonal=True, m=7, trace=True, error_action='ignore', suppress_warnings=True)
print(auto_model.summary())

# Map predictions and calculate total ingredients
ingredients_predicted = ingradients_df.copy()
ingredients_predicted['predicted_quantity'] = ingredients_predicted['pizza_name_id'].map(arima_forecasts).fillna(0)
ingredients_predicted['total_ingredient_qty'] = ingredients_predicted['Items_Qty_In_Grams'] * ingredients_predicted['predicted_quantity']

# Sum total ingredient quantities
ingredient_totals = ingredients_predicted.groupby('pizza_ingredients')['total_ingredient_qty'].sum().to_dict()

print("üçï Total quantity of predicted ingredients for next week:")
print(ingredient_totals)

ingredients_predicted

# predicted ingredients for next week
ingredient_totals



